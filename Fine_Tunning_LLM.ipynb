{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPFz9c8v2m5l"
      },
      "source": [
        "# BERT Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJHxIyFbwop_"
      },
      "source": [
        "# 01 Install & Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AlAXHmjTvu8"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EesffkRC5u9-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWs9dE6CZ8tF"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG6guPrWhimk"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb-FDvhgN4jk"
      },
      "source": [
        "Token: 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"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xImBnrLGrdH"
      },
      "source": [
        "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"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l_KpMoyw_ia"
      },
      "source": [
        "# 02 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-ucqmsMTOux"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Davlan/conll2003_noMISC\")\n",
        "\n",
        "# Tampilkan beberapa sampel dari dataset\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])  # Contoh data pertama dari split \"train\"\n",
        "\n",
        "# Check type of example before loop\n",
        "print(type(dataset[\"train\"][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRq8L_AKPUCq"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcfe1L09iBnn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5Tzb-CKiDT6"
      },
      "outputs": [],
      "source": [
        "label_data = dataset['train']['ner_tags'] # Change 'labels' to 'ner_tags'\n",
        "label_counts = pd.Series(label_data).value_counts()\n",
        "\n",
        "# Print the label counts\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPZ_l6cViPjh"
      },
      "outputs": [],
      "source": [
        "# see the sample\n",
        "dataset['train'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVDczh2NiTuO"
      },
      "outputs": [],
      "source": [
        "# Access the 'ner_tags' column from the 'train' split\n",
        "label_data = dataset['train']['ner_tags']  # Change 'labels' to 'ner_tags'\n",
        "\n",
        "# Flatten the list of lists into a single list of labels\n",
        "all_labels = [label for sublist in label_data for label in sublist]\n",
        "\n",
        "# Count label occurrences using pandas\n",
        "label_counts = pd.Series(all_labels).value_counts()\n",
        "\n",
        "# Print the label counts\n",
        "label_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvL_B-LsiiZZ"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k3DMAw8iewt"
      },
      "outputs": [],
      "source": [
        "for i in range(len(dataset['train'])):\n",
        "    words =dataset['train'][i]['tokens']\n",
        "    labels = dataset['train'][i]['ner_tags']\n",
        "    if len(words) != len(labels):\n",
        "      print(f\"Mismatch found in example {i}:\")\n",
        "      print(\"Words:\", words)\n",
        "      print(\"Labels:\", labels)\n",
        "      print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF5qe8IZipyP"
      },
      "outputs": [],
      "source": [
        "for i in range(len(dataset['test'])):\n",
        "    words = dataset['test'][i]['tokens']\n",
        "    labels = dataset['test'][i]['ner_tags']\n",
        "    if len(words) != len(labels):\n",
        "      print(f\"Mismatch found in example {i}:\")\n",
        "      print(\"Words:\", words)\n",
        "      print(\"Labels:\", labels)\n",
        "      print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6Kdj_pFkYuf"
      },
      "outputs": [],
      "source": [
        "# Dapatkan daftar unik label\n",
        "unique_labels = set(label for seq in dataset[\"train\"][\"ner_tags\"] for label in seq) # Change 'labels' to 'ner_tags'\n",
        "\n",
        "# Buat mapping dari label ke integer\n",
        "label2id = {label: i for i, label in enumerate(sorted(unique_labels))}\n",
        "id2label = {i: label for label, i in label2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0DjEJOFknuy"
      },
      "outputs": [],
      "source": [
        "# Terapkan mapping ke dataset\n",
        "def encode_labels(examples):\n",
        "    # Change 'labels' to 'ner_tags' to access the correct key in the dataset\n",
        "    examples[\"labels\"] = [[label2id[label] for label in seq] for seq in examples[\"ner_tags\"]]\n",
        "    return examples\n",
        "\n",
        "encoded_dataset = dataset.map(encode_labels, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCoPZcauk37x"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "train_df = dataset['train'].to_pandas()\n",
        "\n",
        "train_data, val_data = train_test_split(\n",
        "    train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert the split data back to Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "val_dataset = Dataset.from_dict(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shfBE3bAk6bd"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = {\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": dataset[\"test\"]  # Access the original test set\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMEjozodlR1t"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol4DUA5gqZcK"
      },
      "source": [
        "# Load Tokenizer (Tokenizer dan model BERT, dan Tokenisasi Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnRhBNeGlaW8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Load tokenizer dan model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Tokenisasi dengan label alignment\n",
        "def tokenize_and_align_labels(examples):\n",
        "    # Use 'tokens' instead of 'words' to access the tokenized words\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)  # Ignore token seperti [CLS], [SEP]\n",
        "            else:\n",
        "                label_ids.append(label[word_id])\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_datasets = encoded_dataset.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4NItr6OrzDj"
      },
      "outputs": [],
      "source": [
        "##roberta\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Load tokenizer dan model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 404,
          "referenced_widgets": [
            "eecacd74bdf44ce0be8d6b5fe9c92d76",
            "1e85fc2b2213429a8121b3d9060f701e",
            "1c993631297749f3a4b40d318806d867",
            "1e0e74e08b36486bba734080563e2616",
            "152582b83d8143ccbce8a1610dbab68f",
            "e8c1c5e0b0b44b71b37841b8991f48f4",
            "4b4c77e17a2e4db7b50131e74f28b953",
            "a88d0f185a8d428bae12b1c9ef423e32",
            "6b9ce44dd6874b929534f417b11e2cae",
            "c576788d09504b17878c82eca447b223",
            "a516604ff65d4a25b84f6e63682240f8"
          ]
        },
        "id": "Mm5_F5Fcr2xW",
        "outputId": "ad9fcc7e-f6fb-4b98-a79c-9044ceaedbd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eecacd74bdf44ce0be8d6b5fe9c92d76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-52-3954fb87ce4c>:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1845' max='1845' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1845/1845 12:15, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>0.062017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.045600</td>\n",
              "      <td>0.051108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.033900</td>\n",
              "      <td>0.049970</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1845, training_loss=0.07180381247667762, metrics={'train_runtime': 735.819, 'train_samples_per_second': 40.07, 'train_steps_per_second': 2.507, 'total_flos': 2019194595055704.0, 'train_loss': 0.07180381247667762, 'epoch': 3.0})"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# Load Roberta tokenizer and model\n",
        "# Load Roberta tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True) # Add add_prefix_space=True here\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Tokenize and align labels with Roberta tokenizer\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True) # Add padding=True\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)  # Ignore token seperti [CLS], [SEP]\n",
        "            else:\n",
        "                label_ids.append(label[word_id])\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply tokenization and label alignment to the dataset\n",
        "tokenized_datasets = encoded_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "tokenized_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "tokenized_datasets = DatasetDict({\n",
        "    \"train\": tokenized_datasets[\"train\"],\n",
        "    \"test\": tokenized_datasets[\"test\"]\n",
        "})\n",
        "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].train_test_split(test_size=0.125, seed=42)\n",
        "tokenized_datasets = DatasetDict({\n",
        "    \"train\": tokenized_datasets[\"train\"][\"train\"],\n",
        "    \"validation\": tokenized_datasets[\"train\"][\"test\"],\n",
        "    \"test\": tokenized_datasets[\"test\"]\n",
        "})\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./roberta-finetuned-pos\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Create data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"], # Use validation set for evaluation\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAv7X2rUwJhI"
      },
      "source": [
        "b35c93d536538065ec5cd0c614cca2b779b07fc7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO_vlfV32CP9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "78ff8b27-79f0-4e1e-8a50-339b341efde3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1238' max='1845' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1238/1845 08:04 < 03:57, 2.55 it/s, Epoch 2.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.030300</td>\n",
              "      <td>0.055821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.018100</td>\n",
              "      <td>0.054065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1736' max='1845' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1736/1845 11:05 < 00:41, 2.60 it/s, Epoch 2.82/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.030300</td>\n",
              "      <td>0.055821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.018100</td>\n",
              "      <td>0.054065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# prompt: how to show performance like accuracy, precision, recall, and f1\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=2)\n",
        "\n",
        "# Remove -100 labels from both predicted and true labels\n",
        "true_labels = [[label for label, pred_label in zip(example_labels, pred_labels) if label != -100]\n",
        "               for example_labels, pred_labels in zip(tokenized_datasets[\"test\"][\"labels\"], predicted_labels)]\n",
        "predicted_labels = [[pred_label for label, pred_label in zip(example_labels, pred_labels) if label != -100]\n",
        "                    for example_labels, pred_labels in zip(tokenized_datasets[\"test\"][\"labels\"], predicted_labels)]\n",
        "\n",
        "# Flatten the lists for sklearn metrics\n",
        "true_labels_flat = [label for sublist in true_labels for label in sublist]\n",
        "predicted_labels_flat = [label for sublist in predicted_labels for label in sublist]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels_flat, predicted_labels_flat)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, predicted_labels_flat, average='weighted')\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6Nr7O3c2kM4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "from datasets import DatasetDict # Corrected import\n",
        "\n",
        "# Load the dataset if it hasn't been loaded already\n",
        "dataset = load_dataset(\"Davlan/conll2003_noMISC\")\n",
        "\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.125, seed=42)\n",
        "\n",
        "# 80% of 80% is the 64% for training set\n",
        "dataset = DatasetDict({ # Use corrected name\n",
        "    \"train\": split_dataset[\"train\"],  # Access the 'train' split from the result\n",
        "    \"validation\": split_dataset[\"test\"],  # Access the 'test' split (validation) from the result\n",
        "    \"test\": dataset[\"test\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7ZXV6sR2k-t"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator  # Data Collator menangani padding otomatis\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pbcHA-E2oMh"
      },
      "outputs": [],
      "source": [
        "#\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",         # Directory to save model checkpoints\n",
        "    evaluation_strategy=\"epoch\",   # Evaluate at the end of each epoch\n",
        "    learning_rate=5e-5,            # Learning rate\n",
        "    per_device_train_batch_size=32, # Batch size per GPU/CPU\n",
        "    per_device_eval_batch_size=32, # Batch size for evaluation\n",
        "    num_train_epochs=3,            # Number of training epochs\n",
        "    weight_decay=0.01,             # Weight decay for regularization\n",
        "    save_total_limit=2,            # Limit the number of saved checkpoints\n",
        "    logging_dir=\"./logs\",          # Directory for logs\n",
        "    logging_steps=100,             # Log every 100 steps\n",
        "    load_best_model_at_end=True,   # Load the best model at the end of training\n",
        "    metric_for_best_model=\"accuracy\", # Metric to use for determining the best model\n",
        "    save_strategy=\"epoch\"  # Change save_strategy to \"epoch\" to match evaluation_strategy\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYN9bxtv2qzL"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate  # Install the evaluate package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTjdh73D2sUW"
      },
      "outputs": [],
      "source": [
        "from evaluate import load # Import load_metric from evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf-vVvJ42vEY"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval # Install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLv9baKa2w4t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from evaluate import load # Import load from evaluate\n",
        "\n",
        "# Load metric for evaluation\n",
        "metric = load(\"seqeval\")\n",
        "\n",
        "# Define compute_metrics function\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (-100)\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(predictions, labels)\n",
        "    ]\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHnIGh8b2zc1"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.125, seed=42)\n",
        "\n",
        "# 80% of 80% is the 64% for training set\n",
        "dataset = DatasetDict({ # Changed datasetsDict to DatasetDict\n",
        "    \"train\": split_dataset[\"train\"],  # Access the 'train' split from the result\n",
        "    \"validation\": split_dataset[\"test\"],  # Access the 'test' split (validation) from the result\n",
        "    \"test\": dataset[\"test\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa5e9UwS205-"
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules and classes from the 'transformers' library\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
        "\n",
        "# Now you can initialize the Trainer:\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],  # This should work now\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v9AOP9622mf"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRsGFByo24ET"
      },
      "outputs": [],
      "source": [
        "#\n",
        "import numpy as np\n",
        "\n",
        "# Generate predictions on the validation dataset\n",
        "raw_predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
        "\n",
        "# Convert raw predictions to the most likely labels\n",
        "predictions = np.argmax(raw_predictions, axis=2)\n",
        "\n",
        "# Remove padding and special tokens\n",
        "true_labels = [\n",
        "    [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "    for pred, label in zip(predictions, labels)\n",
        "]\n",
        "true_predictions = [\n",
        "    [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "    for pred, label in zip(predictions, labels)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6gdiXEn29oK"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Flatten the lists\n",
        "flat_true_labels = [label for seq in true_labels for label in seq]\n",
        "flat_predicted_labels = [label for seq in true_predictions for label in seq]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8lJAL-a3AXI"
      },
      "outputs": [],
      "source": [
        "#\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(flat_true_labels, flat_predicted_labels, labels=list(label2id.keys()))\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(flat_true_labels, flat_predicted_labels, labels=list(label2id.keys())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9Is0Re53C5N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=label2id.keys(), yticklabels=label2id.keys(), cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix for POS Tagging\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-yzIuVO3FLk"
      },
      "outputs": [],
      "source": [
        "# prompt: give codes to implement peft using LoRA\n",
        "\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"], # Target modules for LoRA\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.TOKEN_CLS\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# The rest of your training code remains the same, using the 'model' with LoRA applied.\n",
        "\n",
        "# Example (replace with your actual training loop)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],  # This should work now\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaXWYi38n33J"
      },
      "source": [
        "**************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGYx1XHDn72s"
      },
      "source": [
        "### BATASSS"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "152582b83d8143ccbce8a1610dbab68f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c993631297749f3a4b40d318806d867": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a88d0f185a8d428bae12b1c9ef423e32",
            "max": 3250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b9ce44dd6874b929534f417b11e2cae",
            "value": 3250
          }
        },
        "1e0e74e08b36486bba734080563e2616": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c576788d09504b17878c82eca447b223",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a516604ff65d4a25b84f6e63682240f8",
            "value": "â€‡3250/3250â€‡[00:01&lt;00:00,â€‡2187.17â€‡examples/s]"
          }
        },
        "1e85fc2b2213429a8121b3d9060f701e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8c1c5e0b0b44b71b37841b8991f48f4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4b4c77e17a2e4db7b50131e74f28b953",
            "value": "Map:â€‡100%"
          }
        },
        "4b4c77e17a2e4db7b50131e74f28b953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b9ce44dd6874b929534f417b11e2cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a516604ff65d4a25b84f6e63682240f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a88d0f185a8d428bae12b1c9ef423e32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c576788d09504b17878c82eca447b223": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8c1c5e0b0b44b71b37841b8991f48f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eecacd74bdf44ce0be8d6b5fe9c92d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e85fc2b2213429a8121b3d9060f701e",
              "IPY_MODEL_1c993631297749f3a4b40d318806d867",
              "IPY_MODEL_1e0e74e08b36486bba734080563e2616"
            ],
            "layout": "IPY_MODEL_152582b83d8143ccbce8a1610dbab68f"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}